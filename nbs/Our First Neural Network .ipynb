{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets build out first Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dependencies for this notebook is listed in the requirements.txt file. One parent above the nbs directory. This list will keep changing as we add to it so be sure to rerun this line after every git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets declare our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ! mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: kaggle: command not found\r\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: kaggle: command not found\r\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download -d ronitf/heart-disease-uci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_size=2, out_size=2, hidden_size=3):\n",
    "\n",
    "        super(MyFirstNeuralNetwork, self).__init__()\n",
    "\n",
    "        # Set the dimensionality of the network\n",
    "        self.input_size = in_size\n",
    "        self.output_size = out_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize our weights\n",
    "        self._init_weights()\n",
    "\n",
    "    '''\n",
    "    Initialize the weights\n",
    "    '''\n",
    "    def _init_weights(self):\n",
    "        # Create an input tensor of shape (in_size, hidden_size)\n",
    "        self.W_Input = torch.randn(self.input_size, self.hidden_size)\n",
    "        # Create an output tensor of shape (3, 1)\n",
    "        self.W_Output = torch.randn(self.hidden_size, self.output_size)\n",
    "        \n",
    "        print(self.W_Input.shape, self.W_Output.shape)\n",
    "\n",
    "    '''\n",
    "    Create the forward pass\n",
    "    '''\n",
    "    def forward(self, inputs):\n",
    "        # Lets get the element wise dot product\n",
    "        self.z = torch.matmul(inputs, self.W_Input)\n",
    "        # We call the activation\n",
    "        self.state = self._activation(self.z)\n",
    "        # Pass it through the hidden layer\n",
    "        self.z_hidden = torch.matmul(self.state, self.W_Output)\n",
    "        # Finally activate the output\n",
    "        output = self._activation(self.z_hidden)\n",
    "        # Return the output\n",
    "        print(self.z.shape, self.state.shape, self.z_hidden.shape, output.shape)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    '''\n",
    "    Backpropagation algorithm implemented\n",
    "    '''\n",
    "    def backward(self, inputs, labels, output):\n",
    "        # What is the error in output\n",
    "        self.loss = labels - output\n",
    "        # What is the delta loss based on the derivative\n",
    "        self.loss_delta = self.loss * self._derivative(output)\n",
    "        # Get the loss for the existing output weight\n",
    "        print(self.loss_delta.shape, torch.t(self.W_Output).shape)\n",
    "        \n",
    "        self.z_loss = torch.matmul(self.loss_delta, torch.t(self.W_Output))\n",
    "        # Compute the delta like before\n",
    "        self.z_loss_delta = self.z_loss * self._derivative(self.state)\n",
    "        # Finally propogate this to our existing weight tensors to update\n",
    "        # the gradient loss\n",
    "        self.W_Input += torch.matmul(torch.t(inputs), self.z_loss_delta)\n",
    "        self.W_Output += torch.matmul(torch.t(self.state), self.loss_delta)\n",
    "\n",
    "    '''\n",
    "    Here we train the network\n",
    "    '''\n",
    "    def train(self, inputs, labels):\n",
    "        # First we do the foward pass\n",
    "        outputs = self.forward(inputs)\n",
    "        # Then we do the backwards pass\n",
    "        self.backward(inputs, labels, outputs)\n",
    "\n",
    "    '''\n",
    "    Here we perform inference\n",
    "    '''\n",
    "    def predict(self, inputs):\n",
    "        pass\n",
    "\n",
    "    '''\n",
    "    Here we save the model\n",
    "    '''\n",
    "    def save(self, out_path):\n",
    "        self.save(out_path)\n",
    "    \n",
    "    '''\n",
    "    Our non-linear activation function\n",
    "    '''\n",
    "    def _activation(self, s):\n",
    "        # Lets use sigmoid\n",
    "        return 1 / (1 * torch.exp(-s))\n",
    "\n",
    "    '''\n",
    "    Our derivative function used for backpropagation\n",
    "    Usually the sigmoid prime\n",
    "    '''\n",
    "    def _derivative(self, s):\n",
    "        # derivative of sigmoid\n",
    "        return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>199</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>283</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>340</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>226</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>239</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0    63    1   3       145   233    1        0      150      0      2.3   \n",
       "1    37    1   2       130   250    0        1      187      0      3.5   \n",
       "2    41    0   1       130   204    0        0      172      0      1.4   \n",
       "3    56    1   1       120   236    0        1      178      0      0.8   \n",
       "4    57    0   0       120   354    0        1      163      1      0.6   \n",
       "5    57    1   0       140   192    0        1      148      0      0.4   \n",
       "6    56    0   1       140   294    0        0      153      0      1.3   \n",
       "7    44    1   1       120   263    0        1      173      0      0.0   \n",
       "8    52    1   2       172   199    1        1      162      0      0.5   \n",
       "9    57    1   2       150   168    0        1      174      0      1.6   \n",
       "10   54    1   0       140   239    0        1      160      0      1.2   \n",
       "11   48    0   2       130   275    0        1      139      0      0.2   \n",
       "12   49    1   1       130   266    0        1      171      0      0.6   \n",
       "13   64    1   3       110   211    0        0      144      1      1.8   \n",
       "14   58    0   3       150   283    1        0      162      0      1.0   \n",
       "15   50    0   2       120   219    0        1      158      0      1.6   \n",
       "16   58    0   2       120   340    0        1      172      0      0.0   \n",
       "17   66    0   3       150   226    0        1      114      0      2.6   \n",
       "18   43    1   0       150   247    0        1      171      0      1.5   \n",
       "19   69    0   3       140   239    0        1      151      0      1.8   \n",
       "\n",
       "    slope  ca  thal  target  \n",
       "0       0   0     1       1  \n",
       "1       0   0     2       1  \n",
       "2       2   0     2       1  \n",
       "3       2   0     2       1  \n",
       "4       2   0     2       1  \n",
       "5       1   0     1       1  \n",
       "6       1   0     2       1  \n",
       "7       2   0     3       1  \n",
       "8       2   0     3       1  \n",
       "9       2   0     2       1  \n",
       "10      2   0     2       1  \n",
       "11      2   0     2       1  \n",
       "12      2   0     2       1  \n",
       "13      1   0     2       1  \n",
       "14      2   0     2       1  \n",
       "15      1   0     2       1  \n",
       "16      2   0     2       1  \n",
       "17      0   0     2       1  \n",
       "18      2   0     2       1  \n",
       "19      2   2     2       1  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split out dataset between inputs and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "y = df['target']\n",
    "X = df.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a test and train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we transform features by scaling each feature to a given range.\n",
    "This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor((np.array(y_train.values,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227, 13) torch.Size([1, 227])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 3]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "nn = MyFirstNeuralNetwork(in_size=X_train.shape[1], out_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our neural network with 1000 epochs (training loops) and we measure the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([227, 3]) torch.Size([227, 3]) torch.Size([227, 2]) torch.Size([227, 2])\n",
      "tensor([[5.2954e-02, 3.9217e-04],\n",
      "        [1.7577e+00, 7.4750e-01],\n",
      "        [9.1245e-01, 4.6959e-01],\n",
      "        [2.8975e+00, 4.1316e-01],\n",
      "        [1.6487e+00, 9.1777e-01],\n",
      "        [1.4421e+00, 7.8686e-01],\n",
      "        [1.7682e+00, 6.0586e-01],\n",
      "        [3.3079e-01, 3.9138e-02],\n",
      "        [1.5003e-01, 3.0300e-05],\n",
      "        [1.8591e-03, 1.1757e-07],\n",
      "        [1.3428e+00, 4.1034e-01],\n",
      "        [3.3279e-01, 5.0488e-02],\n",
      "        [7.0771e-27, 0.0000e+00],\n",
      "        [7.9000e-01, 3.5713e-01],\n",
      "        [2.6576e+00, 1.5059e-01],\n",
      "        [6.9164e+00, 1.1266e+00],\n",
      "        [7.3010e-01, 2.9211e-01],\n",
      "        [1.2784e+00, 8.7293e-01],\n",
      "        [9.0858e-01, 5.9425e-01],\n",
      "        [1.1721e+00, 8.1714e-01],\n",
      "        [7.4320e-01, 9.3856e-02],\n",
      "        [7.8127e-04, 2.3504e-08],\n",
      "        [1.3018e+00, 8.8582e-01],\n",
      "        [1.0650e+00, 1.8940e-01],\n",
      "        [1.2064e+00, 6.8964e-01],\n",
      "        [1.1241e+00, 7.8098e-01],\n",
      "        [1.5326e+00, 7.9939e-01],\n",
      "        [1.2114e+00, 9.6101e-01],\n",
      "        [1.2802e+00, 5.8765e-01],\n",
      "        [1.2978e+00, 6.8319e-01],\n",
      "        [2.0367e+00, 1.3326e-01],\n",
      "        [7.9279e+00, 1.5456e+00],\n",
      "        [1.6506e+00, 1.0424e+00],\n",
      "        [1.1241e+00, 8.8453e-01],\n",
      "        [9.4324e-01, 5.6743e-01],\n",
      "        [7.4371e-01, 2.2901e-01],\n",
      "        [2.6552e-15, 3.6331e-35],\n",
      "        [1.8317e-01, 1.2695e-02],\n",
      "        [2.8209e-01, 2.3085e-02],\n",
      "        [6.5311e-01, 7.3924e-03],\n",
      "        [1.1068e+00, 8.5489e-01],\n",
      "        [7.9293e-01, 3.1345e-01],\n",
      "        [1.2500e+00, 8.3561e-01],\n",
      "        [1.8312e+00, 4.2560e-01],\n",
      "        [1.5199e+00, 3.9579e-01],\n",
      "        [1.4180e+00, 8.2400e-01],\n",
      "        [1.3543e+00, 9.0336e-01],\n",
      "        [3.2050e-01, 2.6934e-02],\n",
      "        [1.3396e+00, 8.3437e-01],\n",
      "        [1.2522e+00, 7.5870e-01],\n",
      "        [1.5763e+00, 1.6494e-03],\n",
      "        [9.9329e-01, 1.8294e-01],\n",
      "        [1.0519e+00, 6.4814e-01],\n",
      "        [1.9989e+00, 6.9075e-01],\n",
      "        [6.1225e-01, 1.4551e-01],\n",
      "        [6.7126e-01, 1.2374e-01],\n",
      "        [1.4391e+00, 1.0163e+00],\n",
      "        [1.4173e+00, 8.8678e-01],\n",
      "        [1.7512e+00, 6.4358e-01],\n",
      "        [1.0421e+00, 2.8721e-01],\n",
      "        [1.1636e+01, 1.4610e+00],\n",
      "        [8.6468e-01, 2.1793e-01],\n",
      "        [2.1693e+00, 2.5510e-01],\n",
      "        [2.4444e-01, 1.8395e-02],\n",
      "        [1.9534e+00, 2.9285e-01],\n",
      "        [1.4413e+00, 1.0671e+00],\n",
      "        [2.1830e-01, 1.6525e-02],\n",
      "        [1.3116e-01, 3.4212e-03],\n",
      "        [3.2089e+00, 1.3838e+00],\n",
      "        [1.1492e+00, 8.9345e-01],\n",
      "        [1.9686e+00, 5.6972e-01],\n",
      "        [1.4160e+00, 7.0576e-01],\n",
      "        [1.2847e+00, 8.7662e-01],\n",
      "        [8.9802e-01, 2.1010e-01],\n",
      "        [1.1079e+00, 7.7003e-01],\n",
      "        [3.9019e-03, 1.4452e-06],\n",
      "        [8.2792e-01, 4.6417e-01],\n",
      "        [2.0921e+00, 4.7166e-01],\n",
      "        [1.5732e+00, 1.0628e+00],\n",
      "        [6.4263e+00, 4.3671e-01],\n",
      "        [1.6362e+00, 5.0253e-01],\n",
      "        [2.5695e-01, 1.4056e-02],\n",
      "        [2.4238e-01, 1.9123e-02],\n",
      "        [1.3699e+00, 5.5580e-01],\n",
      "        [1.3621e+00, 4.9122e-01],\n",
      "        [7.6327e-01, 3.8215e-01],\n",
      "        [1.8754e+00, 7.4239e-01],\n",
      "        [2.4329e+00, 1.0687e+00],\n",
      "        [1.6018e+00, 7.7222e-01],\n",
      "        [1.6939e+00, 7.6320e-01],\n",
      "        [3.5440e-07, 1.9459e-16],\n",
      "        [6.0558e-01, 1.8713e-01],\n",
      "        [1.0458e+00, 8.0524e-01],\n",
      "        [1.7711e+00, 3.5358e-01],\n",
      "        [1.4741e+00, 8.2522e-01],\n",
      "        [1.1610e+00, 9.0893e-01],\n",
      "        [5.1771e-01, 3.9833e-04],\n",
      "        [1.1751e+00, 5.7804e-01],\n",
      "        [1.0799e+00, 6.3996e-01],\n",
      "        [7.3201e-01, 2.9738e-01],\n",
      "        [4.4168e-01, 1.0565e-01],\n",
      "        [5.0787e+00, 4.2139e-01],\n",
      "        [1.3678e+00, 5.9505e-01],\n",
      "        [2.0142e-01, 1.1268e-02],\n",
      "        [2.1371e+00, 2.8174e-01],\n",
      "        [2.2444e+01, 1.3345e-07],\n",
      "        [1.2110e+00, 5.8788e-01],\n",
      "        [7.2364e-01, 3.7020e-01],\n",
      "        [1.2535e+00, 8.7075e-01],\n",
      "        [4.4291e+02, 4.9821e+00],\n",
      "        [2.5794e-01, 1.7305e-02],\n",
      "        [1.8383e+00, 4.2153e-01],\n",
      "        [7.6388e+01, 2.9884e+00],\n",
      "        [2.5813e+00, 1.2749e+00],\n",
      "        [1.3316e+00, 6.8880e-01],\n",
      "        [1.5097e+00, 6.2289e-01],\n",
      "        [1.6701e+01, 2.2301e+00],\n",
      "        [7.2504e-01, 7.2682e-02],\n",
      "        [9.3975e-01, 4.8688e-01],\n",
      "        [1.4793e+00, 1.7882e-02],\n",
      "        [1.0048e+00, 3.4878e-01],\n",
      "        [1.0219e+00, 6.8353e-01],\n",
      "        [3.6594e-01, 5.4355e-02],\n",
      "        [2.7490e+00, 5.2654e-01],\n",
      "        [9.5615e-01, 5.3066e-01],\n",
      "        [5.4091e-01, 1.3408e-01],\n",
      "        [1.8081e-01, 9.2811e-03],\n",
      "        [1.2159e+00, 8.2534e-01],\n",
      "        [1.1775e+00, 7.7451e-01],\n",
      "        [6.8734e-01, 2.0719e-01],\n",
      "        [4.3669e+00, 4.2983e-02],\n",
      "        [1.1036e+00, 5.9252e-01],\n",
      "        [1.2795e+00, 5.4528e-01],\n",
      "        [2.7031e-01, 3.4374e-03],\n",
      "        [1.7967e+00, 1.0049e+00],\n",
      "        [1.1443e+00, 7.6291e-01],\n",
      "        [3.5272e+00, 5.0018e-01],\n",
      "        [1.1568e+00, 2.3137e-02],\n",
      "        [6.8711e-01, 2.7983e-01],\n",
      "        [6.7720e-02, 8.0302e-04],\n",
      "        [1.5294e+00, 8.1165e-01],\n",
      "        [6.2088e-01, 1.6498e-01],\n",
      "        [2.4444e-01, 1.8395e-02],\n",
      "        [8.4160e-01, 4.4620e-01],\n",
      "        [1.0896e+02, 4.1163e+00],\n",
      "        [1.2132e+00, 9.0386e-01],\n",
      "        [9.7589e-13, 2.0723e-29],\n",
      "        [8.9130e-01, 5.3946e-01],\n",
      "        [2.8823e+00, 3.3817e-01],\n",
      "        [1.0931e+00, 6.4272e-01],\n",
      "        [3.4334e-01, 5.0634e-02],\n",
      "        [1.1041e+00, 6.3300e-01],\n",
      "        [7.9695e-02, 1.1498e-03],\n",
      "        [4.7329e-01, 3.0762e-02],\n",
      "        [1.1282e+00, 7.6319e-01],\n",
      "        [9.9712e-01, 6.4561e-01],\n",
      "        [1.2370e+00, 3.8419e-01],\n",
      "        [1.2576e+00, 7.8828e-01],\n",
      "        [9.7302e-02, 2.6112e-03],\n",
      "        [1.2771e+00, 7.4402e-01],\n",
      "        [1.9934e+00, 1.1931e+00],\n",
      "        [7.5782e-01, 4.0177e-01],\n",
      "        [5.5644e-01, 1.5443e-01],\n",
      "        [2.8857e+00, 2.1200e-01],\n",
      "        [1.5344e+00, 8.1863e-01],\n",
      "        [8.1166e-01, 3.0202e-01],\n",
      "        [8.8602e-01, 5.5993e-01],\n",
      "        [3.1401e+00, 1.3635e+00],\n",
      "        [1.4032e+01, 2.3832e-01],\n",
      "        [4.7815e-01, 1.0746e-01],\n",
      "        [1.2055e+00, 8.4924e-01],\n",
      "        [1.9849e-01, 9.3769e-03],\n",
      "        [6.7851e-01, 1.9453e-01],\n",
      "        [1.2083e+00, 7.3495e-01],\n",
      "        [8.5726e-01, 2.4467e-01],\n",
      "        [1.0757e+00, 5.7095e-01],\n",
      "        [3.6581e+00, 2.6555e-01],\n",
      "        [2.3958e+00, 1.1005e+00],\n",
      "        [1.0509e+00, 7.4065e-01],\n",
      "        [5.7296e-02, 7.2314e-04],\n",
      "        [1.4317e+00, 6.9328e-01],\n",
      "        [1.4096e+00, 8.3949e-01],\n",
      "        [8.2079e-01, 4.0566e-01],\n",
      "        [1.8110e+00, 7.0781e-01],\n",
      "        [9.5788e-01, 5.5331e-01],\n",
      "        [3.0611e-02, 6.0979e-05],\n",
      "        [1.0315e+00, 3.5095e-01],\n",
      "        [1.0878e+00, 7.8388e-01],\n",
      "        [1.0525e+00, 8.0437e-01],\n",
      "        [1.1858e+01, 1.6853e+00],\n",
      "        [4.8349e-01, 1.0669e-01],\n",
      "        [1.3680e+00, 8.9441e-01],\n",
      "        [1.1830e+00, 9.6946e-01],\n",
      "        [5.6459e-01, 3.5024e-02],\n",
      "        [1.2477e+00, 8.8623e-01],\n",
      "        [1.3856e+00, 7.6608e-01],\n",
      "        [1.0869e+00, 7.5911e-01],\n",
      "        [2.4568e+01, 2.7277e+00],\n",
      "        [8.8953e-01, 4.7152e-01],\n",
      "        [1.0927e+00, 5.6040e-01],\n",
      "        [1.0529e+00, 5.3697e-01],\n",
      "        [3.5976e-03, 4.4416e-08],\n",
      "        [1.2753e+00, 4.5860e-01],\n",
      "        [1.0474e+00, 7.1174e-01],\n",
      "        [1.0092e+00, 4.3067e-01],\n",
      "        [8.7958e-01, 4.7017e-01],\n",
      "        [9.9565e-01, 7.5316e-01],\n",
      "        [3.1198e+00, 3.3404e-01],\n",
      "        [1.2210e+00, 9.2602e-01],\n",
      "        [3.5298e+00, 1.4205e+00],\n",
      "        [1.8509e+00, 1.1089e+00],\n",
      "        [1.2435e+00, 7.3140e-01],\n",
      "        [6.8984e-01, 3.1827e-01],\n",
      "        [1.7960e+00, 5.8574e-01],\n",
      "        [1.8729e-01, 8.5167e-03],\n",
      "        [7.0465e+01, 1.5616e+00],\n",
      "        [1.1996e+00, 8.6216e-01],\n",
      "        [5.1048e-01, 1.3424e-01],\n",
      "        [5.2767e-04, 8.1722e-09],\n",
      "        [7.9798e-01, 3.9023e-01],\n",
      "        [1.0687e+00, 7.2107e-01],\n",
      "        [4.0717e-01, 8.0919e-02],\n",
      "        [1.3977e+00, 1.0124e+00],\n",
      "        [1.7243e+00, 6.2169e-01],\n",
      "        [9.4290e-01, 4.2016e-01],\n",
      "        [1.8594e+01, 1.7416e-01],\n",
      "        [8.0620e-01, 2.2068e-01]])\n",
      "Loss: inf\n",
      "torch.Size([227, 3]) torch.Size([227, 3]) torch.Size([227, 2]) torch.Size([227, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (227) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-f192a93bddb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#loss = torch.mean((labels - outputs)**2).detach().item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-137-06d73b99b546>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Then we do the backwards pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     '''\n",
      "\u001b[0;32m<ipython-input-137-06d73b99b546>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, inputs, labels, output)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# What is the error in output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m# What is the delta loss based on the derivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (227) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    outputs = nn(X_train)\n",
    "    loss = torch.mean((labels - outputs)**2).detach().item()\n",
    "    print(\"Loss: {}\".format(loss))\n",
    "    nn.train(X_train, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to initialize the weights with something better. Hint (Xavier Initialization)\n",
    "2. Add a bias to the forward pass. Recall the affine transform is (inputs . weights) + bias\n",
    "3. We are missing a learning rate to the backwards pass. See if you can add that in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How would we rewrite this code using PyTorch built-in methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch gives us most of this functionality out of the box. First we can flag all Tensors to use Autograd. You can read more about autograd here: https://pytorch.org/docs/stable/autograd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelz= torch.tensor(([1],[0],[0],[1]), dtype=torch.float)\n",
    "inputs = torch.tensor(([20, 90],[10, 20],[30, 40],[20, 50]), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
