{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets build out first Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All dependencies for this notebook is listed in the requirements.txt file. One parent above the nbs directory. This list will keep changing as we add to it so be sure to rerun this line after every git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets declare our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets download -d ronitf/heart-disease-uci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_size=2, out_size=2, hidden_size=3):\n",
    "\n",
    "        super(MyFirstNeuralNetwork, self).__init__()\n",
    "\n",
    "        # Set the dimensionality of the network\n",
    "        self.input_size = in_size\n",
    "        self.output_size = out_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = 0.5\n",
    "\n",
    "        # Initialize our weights\n",
    "        self._init_weights()\n",
    "\n",
    "    '''\n",
    "    Initialize the weights\n",
    "    '''\n",
    "    def _init_weights(self):\n",
    "        # Create an input tensor of shape (in_size, hidden_size)\n",
    "        self.W_Input = torch.randn(self.input_size, self.hidden_size)\n",
    "        # Create an output tensor of shape (3, 1)\n",
    "        self.W_Output = torch.randn(self.hidden_size, self.output_size)\n",
    "        \n",
    "        self.bias = torch.randn((self.hidden_size))\n",
    "        self.bias_hidden = torch.randn((self.hidden_size))\n",
    "        self.bias_out = torch.randn((self.output_size))\n",
    "    '''\n",
    "    Create the forward pass\n",
    "    '''\n",
    "    def forward(self, inputs):\n",
    "        # Lets get the element wise dot product\n",
    "        self.z = torch.matmul(inputs, self.W_Input) + self.bias\n",
    "        # We call the activation\n",
    "        self.state = self._activation(self.z) + self.bias_hidden\n",
    "        # Pass it through the hidden layer\n",
    "        self.z_hidden = torch.matmul(self.state, self.W_Output)\n",
    "        # Finally activate the output\n",
    "        output = self._activation(self.z_hidden) + self.bias_out\n",
    "        # Return the output\n",
    "        return output\n",
    "\n",
    "    '''\n",
    "    Backpropagation algorithm implemented\n",
    "    '''\n",
    "    def backward(self, inputs, labels, output):\n",
    "        # What is the error in output\n",
    "        self.loss = labels - output\n",
    "        # What is the delta loss based on the derivative\n",
    "        self.loss_delta = self.learning_rate * self.loss * self._derivative(output)\n",
    "        # Get the loss for the existing output weight\n",
    "        self.z_loss = torch.matmul(self.loss_delta, torch.t(self.W_Output))\n",
    "        # Compute the delta like before\n",
    "        self.z_loss_delta = self.learning_rate * self.z_loss * self._derivative(self.state)\n",
    "        # Finally propogate this to our existing weight tensors to update\n",
    "        # the gradient loss\n",
    "        self.W_Input += torch.matmul(torch.t(inputs), self.z_loss_delta)\n",
    "        self.W_Output += torch.matmul(torch.t(self.state), self.loss_delta)\n",
    "\n",
    "    '''\n",
    "    Here we train the network\n",
    "    '''\n",
    "    def train(self, inputs, labels):\n",
    "        # First we do the foward pass\n",
    "        outputs = self.forward(inputs)\n",
    "        # Then we do the backwards pass\n",
    "        self.backward(inputs, labels, outputs)\n",
    "\n",
    "    '''\n",
    "    Here we perform inference\n",
    "    '''\n",
    "    def predict(self, inputs):\n",
    "        pass\n",
    "\n",
    "    '''\n",
    "    Here we save the model\n",
    "    '''\n",
    "    def save(self, out_path):\n",
    "        self.save(out_path)\n",
    "    \n",
    "    '''\n",
    "    Our non-linear activation function\n",
    "    '''\n",
    "    def _activation(self, s):\n",
    "        # Lets use sigmoid\n",
    "        return 1 / (1 * torch.exp(-s))\n",
    "\n",
    "    '''\n",
    "    Our derivative function used for backpropagation\n",
    "    Usually the sigmoid prime\n",
    "    '''\n",
    "    def _derivative(self, s):\n",
    "        # derivative of sigmoid\n",
    "        return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "sc = MinMaxScaler((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split out dataset between inputs and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "y = df['target']\n",
    "X = df.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a test and train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we transform features by scaling each feature to a given range.\n",
    "This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor((y_train.values,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MyFirstNeuralNetwork(in_size=X_train.shape[1], out_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our neural network with 1000 epochs (training loops) and we measure the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    outputs = nn(X_train)\n",
    "    loss = torch.mean((y_train - outputs)**2).detach().item()\n",
    "    tqdm.write(\"Loss: {}\".format(loss))\n",
    "    nn.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try to initialize the weights with something better. Hint (Xavier Initialization)\n",
    "2. Add a bias to the forward pass. Recall the affine transform is (inputs . weights) + bias\n",
    "3. We are missing a learning rate to the backwards pass. See if you can add that in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How would we rewrite this code using PyTorch built-in methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch gives us most of this functionality out of the box. First we can flag all Tensors to use Autograd. You can read more about autograd here: https://pytorch.org/docs/stable/autograd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "# Populate the best\n",
    "X_train = torch.tensor(sc.fit_transform(X_train), dtype=torch.float, requires_grad=True)\n",
    "X_test = torch.tensor(sc.transform(X_test), dtype=torch.float)\n",
    "\n",
    "y_train = torch.tensor(y_train.values)\n",
    "y_test = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first way using the torch.nn.Sequential. In the Sequential model modules will be added to it in the order they are passed in the constructor. This is a quick way to write a small neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = torch.nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(13, 100)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fc2', nn.Linear(100, 100)),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('fc3', nn.Linear(100, 2)),\n",
    "    ('sigmoid', nn.Sigmoid())\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1000)):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    print(\"Epoch {}, Loss: {}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(X_test)\n",
    "_, preds_y = torch.max(prediction, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, preds_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot \n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot([{'y': losses}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_no_back = []\n",
    "for epoch in tqdm(range(1000)):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    optimizer.step()\n",
    "    losses_no_back.append(loss.item())\n",
    "    print(\"Epoch {}, Loss: {}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot([{'y': losses_no_back}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
